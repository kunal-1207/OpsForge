global:
  # The smarthost and SMTP sender used for mail notifications
  # smtp_smarthost: 'localhost:587'
  # smtp_from: 'alertmanager@example.org'
  # smtp_auth_username: 'alertmanager'
  # smtp_auth_password: 'password'

# The root route on which each incoming alert enters
route:
  # The labels by which incoming alerts are grouped together. For example,
  # multiple alerts coming in for cluster=A and alertname=LatencyHigh would
  # be batched into a single group.
  group_by: ['alertname', 'cluster', 'service']

  # When a new group of alerts is created by an incoming alert, wait at
  # least 'group_wait' to send the initial notification.
  # This way ensures that you get multiple alerts for the same group that start
  # firing shortly after another are batched together on the first 
  # notification.
  group_wait: 30s

  # When the first notification was sent, wait 'group_interval' to send an 
  # alert that starts firing for a group of that the first notification was 
  # sent.
  group_interval: 5m

  # If an alert has successfully been sent, wait 'repeat_interval' to
  # resend them.
  repeat_interval: 3h

  # A default receiver
  receiver: default-receiver

  # All the above attributes are inherited by all child routes and can 
  # overwritten on each.

  # The child route trees.
  routes:
  # This routes performs a regular expression match on alert labels to
  # catch alerts that are related to our API service.
  - matchers:
      - service="api-service"
    receiver: api-team
    group_wait: 10s
    repeat_interval: 1h

  # This routes performs a regular expression match on alert labels to
  # catch alerts that are related to our worker service.
  - matchers:
      - service="worker-service"
    receiver: worker-team
    group_wait: 10s
    repeat_interval: 1h

  # This routes performs a regular expression match on alert labels to
  # catch alerts that are critical.
  - matchers:
      - severity="critical"
    receiver: critical-team
    group_wait: 5s
    repeat_interval: 30m

# Inhibition rules allow to mute a set of alerts given that another alert is
# firing.
# We use this to mute any warning-level notifications if the same alert is
# already critical.
inhibit_rules:
- source_matchers:
    - severity="critical"
  target_matchers:
    - severity="warning"
  # Apply inhibition if the alertname is the same.
  equal: ['alertname', 'cluster', 'service']

receivers:
- name: 'default-receiver'
  email_configs:
  - to: 'admin@example.com'
    from: 'alertmanager@example.com'
    smarthost: 'localhost:587'
    auth_username: 'alertmanager'
    auth_identity: 'alertmanager'
    auth_password: 'password'
  # Add webhook for Slack or other notification systems
  # webhook_configs:
  # - url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'

- name: 'api-team'
  email_configs:
  - to: 'api-team@example.com'
    from: 'alertmanager@example.com'
    smarthost: 'localhost:587'
    auth_username: 'alertmanager'
    auth_identity: 'alertmanager'
    auth_password: 'password'

- name: 'worker-team'
  email_configs:
  - to: 'worker-team@example.com'
    from: 'alertmanager@example.com'
    smarthost: 'localhost:587'
    auth_username: 'alertmanager'
    auth_identity: 'alertmanager'
    auth_password: 'password'

- name: 'critical-team'
  email_configs:
  - to: 'sre-team@example.com'
    from: 'alertmanager@example.com'
    smarthost: 'localhost:587'
    auth_username: 'alertmanager'
    auth_identity: 'alertmanager'
    auth_password: 'password'
  # Critical alerts should also trigger immediate notifications
  webhook_configs:
  - url: 'https://hooks.slack.com/services/YOUR/CRITICAL/WEBHOOK'
    send_resolved: true